<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>CAP Theorem</title><link rel="stylesheet" href="epub.css" type="text/css"></link><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></meta></head><body id="page"><div style="display:none;"><a id="GBS.0183.01"></a></div><div title="CAP Theorem" class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"></a>CAP Theorem</h1></div></div></div><p>We&#39;re going to cover a stateless <a id="id432" class="indexterm"></a>worker example in this chapter, but we&#39;re also going to cover state being stored in a cluster to understand the trade-offs of design choices. Eric Brewer&#39;s often cited <span class="emphasis"><em>CAP Theorem</em></span> talks about the compromises of distributed systems and is a helpful model for reasoning about how a system deals with state and exceptional events. CAP Theorem is an acronym<a id="GBS.0183.02"></a> for the three qualities of distributed systems, as described in the following, and the compromise of choosing between them.</p><div title="C – Consistency" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec71"></a>C – Consistency</h2></div></div></div><p>Consistency means <a id="id433" class="indexterm"></a>that a client will return the most recent value for a given record. Consider a bank account&#8212;if you try to withdraw $400 immediately after depositing a $400 cheque, you expect the systems to give you the correct balance and allow you to<a id="GBS.0183.03"></a> withdraw the $400.</p></div><div title="A – Availability" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec72"></a>A – Availability</h2></div></div></div><p>Availability means that a<a id="id434" class="indexterm"></a> non-failing node is able to give a reasonable response (for example, give an accurate picture of whether a write was a success or failure).</p></div><div title="P – Partition Tolerance" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec73"></a>P – Partition Tolerance</h2></div></div></div><p>Partition Tolerance <a id="id435" class="indexterm"></a>means that a system continues to operate normally if a node is removed from the network due to a temporary network failure. If data is replicated<a id="GBS.0183.04"></a> across three nodes and one of the nodes becomes temporarily unavailable, then the system can be said to be partition tolerant if the other two nodes can come to the same conclusion about the most recent record.</p></div><div title="Compromises in CAP Theorem" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec74"></a>Compromises in CAP Theorem</h2></div></div></div><p>CAP theorem is often cited <a id="id436" class="indexterm"></a>as the following statement: of the three&#8212;Consistency, Availability, and Partition tolerance&#8212;that a distributed system can choose<a id="GBS.0183.05"></a> any two. This is a misleading oversimplification, and if you start reading articles, you&#39;ll see a lot of conflicting rebuttals.</p><p>We&#39;ll start by assuming that we want a system that has Partition Tolerance and only talking about the compromises between Availability and Consistency. Why you ask? In a time-bound request, if a node becomes unavailable, you are really deciding between responding with<a id="GBS.0184.01"></a> an error (preferring Consistency) or continuing even though there might be inconsistency between servers (preferring Availability). Waiting around too long means the request will be abandoned so time is a factor at play and a system must make a decision. We&#39;ll look at this a bit more but Eric Brewer also wrote an article 12 years after his Cap Theorem paper that has further discussion on this point:<a id="GBS.0184.02"></a> <a href="http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" class="ulink">http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed</a>:</p><div class="mediaobject"><img src="graphics/B04006_06_02.jpg" alt="Compromises in CAP Theorem"></img><a id="GBS.0184.03"></a></div><div title="CP System – Preferring Consistency" class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec63"></a>CP System – Preferring Consistency</h3></div></div></div><p>There are different <a id="id437" class="indexterm"></a>ways in which a consistent distributed store can be implemented. Perhaps, the simplest example of a strongly consistent data-store would be to have a master node and any number of secondary nodes on which the data is replicated. You always write to the primary node, and to ensure the most recent data is read. Data must always be read from<a id="GBS.0184.04"></a> the primary node as well. In the case of the failure of the master node, the system will no longer be available:</p><div class="mediaobject"><img src="graphics/B04006_06_03.jpg" alt="CP System – Preferring Consistency"></img><a id="GBS.0185.01"></a></div><p>Generally, some sort<a id="id438" class="indexterm"></a> of failover occurs where a secondary node becomes the new primary node. We give up availability because our system can&#39;t be read from or written to in the case of a partition, and instead goes through a failover process to elect the new primary node. Once the failover is complete, availability will be restored. Redis Sentinel, or replicated RDBMSs are great examples of strongly<a id="GBS.0185.02"></a> consistent distributed systems.</p><p>You would choose a consistent system if you need ATOMIC reads/writes, transactions, and so on.</p></div><div title="AP System – Preferring Availability" class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec64"></a>AP System – Preferring Availability</h3></div></div></div><p>A system that prefers <a id="id439" class="indexterm"></a>availability and partition tolerance at the cost of consistency is said to be &quot;eventually consistent.&quot; In highly available distributed data-stores such as Cassandra and Riak, this is a very common model.</p><p>We&#39;ll<a id="GBS.0185.03"></a> look at AP systems in more detail in this chapter because they take a bit of work to reason about, but we can look at one example of how we might implement an AP system. Let&#39;s assume that there are three replicas of the data on three nodes. When we write data, the data is written to one node and, then, it is later replicated across all three nodes. It doesn&#39;t matter which one we write into, that<a id="GBS.0185.04"></a> node will coordinate the write across the remaining nodes:</p><div class="mediaobject"><img src="graphics/B04006_06_04.jpg" alt="AP System – Preferring Availability"></img><a id="GBS.0185.05"></a></div><p>When we read <a id="id440" class="indexterm"></a>from a node, we only need one node to read the data, so our client may choose a random node to read from. In this example, our system is eventually consistent because when we read from a node, and the data may be a bit out of date (not Consistent), but as we can read and write from any of the nodes, our system is both Partition tolerant and highly Available. If a node becomes unavailable,<a id="GBS.0186.01"></a> we can just try another node.</p><div title="Note" style="" class="note"><div class="inner"><h3 class="title"><a id="note03"></a>Note</h3><p>Note that this is very hard to get correct because the time and ordering of events in systems is challenging to determine. Clocks will never be perfectly synchronized across machines so other approaches are taken for ordering such as Vector clocks.</p></div></div></div><div title="Consistency as a Sliding Scale" class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec65"></a>Consistency as a Sliding Scale</h3></div></div></div><p>Practically speaking, the<a id="id441" class="indexterm"></a> choices between the three trade-offs are not toggles<a id="GBS.0186.02"></a> but sliders. In an eventually consistent system, for example, if we have three replicas of a record we&#39;re looking for, we can have low consistency by requiring any of the nodes to give us the record. We can have greater consistency by having any two of the three return with the data. And we can have the greatest consistency by requiring asking all three nodes to provide back a result. When we get<a id="GBS.0186.03"></a> back the data from the nodes, several different mechanisms can be used to order the records, so we&#39;ll choose the newest one. However, doing this can sacrifice partition tolerance. If we need all three replicas to be available, then we can&#39;t tolerate one of the nodes disappearing. Usually, only requiring a quorum or majority of nodes to be up and give a view is a good trade-off between consistency<a id="GBS.0186.04"></a> and partition tolerance.</p><div title="Note" style="" class="note"><div class="inner"><h3 class="title"><a id="tip13"></a>Tip</h3><p>Ordering is its own problem as often clocks are not perfectly synchronized across nodes so other ordering mechanisms are used such as Vector Clocks.</p></div></div><p>Similarly, in a CP system, we can sacrifice some consistency for greater availability by allowing secondary nodes to be read from. If we keep writes on the master, we still have highly consistent writes, but we allow<a id="GBS.0186.05"></a> our reads to be eventually consistent, so our database becomes AP on reads.</p><p>Based on our use <a id="id442" class="indexterm"></a>cases, we can &quot;tune&quot; our CAP stance to best meet our use case needs. It is even possible to use blends of these strategies in the same application in the same data-store for different types of data!</p></div></div></div><div style="display:none;"><a id="GBS.0186.06"></a></div></body></html>