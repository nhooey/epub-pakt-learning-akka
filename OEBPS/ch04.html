<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 4. Actor Lifecycle – Handling State and Failure</title><link rel="stylesheet" href="epub.css" type="text/css"></link><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></meta></head><body id="page"><div style="display:none;"><a id="GBS.0123.01"></a></div><div title="Chapter 4. Actor Lifecycle – Handling State and Failure" class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"></a>Chapter 4. Actor Lifecycle – Handling State and Failure</h1></div></div></div><p>In this chapter, we&#39;re going to cover the actor&#39;s life cycle—what happens when an actor encounters an exceptional state and how we can change its state to modify its behavior. We&#39;ve looked at fault tolerance as a reactive tenet and covered how to store state in an actor—we&#39;re going to expand on those topics here.</p><p>Before we get into failure<a id="GBS.0123.02"></a> and state, we&#39;re going to introduce what is known as The Fallacies of Distributed Computing&#8212;a list of common misconceptions that developers hold about systems communicating over the network.</p><p>This chapter will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li style="list-style-type: disc;" class="listitem">The Fallacies of Distributed Computing</li><li style="list-style-type: disc;" class="listitem">What happens to an actor when it fails</li><li style="list-style-type: disc;" class="listitem">How an Actor can be supervised to handle failure</li><li style="list-style-type: disc;" class="listitem">How an Actor can change behavior with<a id="GBS.0123.03"></a> <code class="literal">become()</code> and as Finite State Machines</li></ul></div><div title="The 8 Fallacies of Distributed Computing" class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"></a>The 8 Fallacies of Distributed Computing</h1></div></div></div><p>Before we look at the examples, we&#39;ll have a quick look at the Fallacies of Distributed Computing. The Fallacies of <a id="id291" class="indexterm"></a>Distributed Computing are a set of incorrect<a id="id292" class="indexterm"></a> assumptions that a team from Sun Microsystems asserted inexperienced developers make about systems working over a network. Let us have a look at the eight<a id="GBS.0123.04"></a> fallacies in detail.</p><div title="The network is reliable" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec460"></a>The network is reliable</h2></div></div></div><p>The fallacy we hear is<a id="id293" class="indexterm"></a> most often cited, and that seems to come into play the most in systems today, is that the network is reliable. It is not. It&#39;s easy to think of a remote system in the same way we would think of a local one&#8212;Akka further helps us with this false assumption through location transparency by raising the level of abstraction when<a id="GBS.0123.05"></a> talking over the network&#8212;and we interact with remote actors exactly as we would with local ones.</p><p>We can reason that a service may be up or down, but what really constitutes a service being available? Even if a service is running, traffic has to be able to make it to the service. If you have two servers on a small network, it&#39;s very easy to think that they will be connected and talking successfully<a id="GBS.0124.01"></a> all of the time. But something will eventually fail between them. A router can crash and restart, the power can fail, someone can unplug the wrong cable in the datacenter, and so on. When you start to scale up, and you&#39;re talking about a thousand or ten thousand servers, the chance of things being unavailable become more common and more likely. In <a id="id294" class="indexterm"></a>
<span class="strong"><strong>Amazon Web Server</strong></span> (<span class="strong"><strong>AWS</strong></span>), it&#39;s actually extremely<a id="GBS.0124.02"></a> common for temporary<a id="id295" class="indexterm"></a> network failures to occur. In the system we are working on today, in AWS, we see disconnects and periods of network failure daily.</p></div><div title="Latency is zero" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec46"></a>Latency is zero</h2></div></div></div><p>It takes time for messages to<a id="id296" class="indexterm"></a> travel over the network and it takes time for responses to come back. A remote Actor will incur a lot more latency overhead than one in local memory. Before any data begins to be sent over the network,<a id="GBS.0124.03"></a> there are several things that happen.</p><p>Hostnames are resolved to IPs (via DNS), IP addresses are resolved to their receiver&#39;s MAC address (via ARP), a TCP connection is established by a handshake, and then data starts to move. Data transferred over TCP is sent in sequenced packets, which receive an acknowledgment back. With this complexity, you can imagine that making a request to Google search<a id="GBS.0124.04"></a> takes some time just to leave your machine and to get to Google.</p><p>Given the overhead of network communication, we can make an assumption that it is better to make less calls with larger messages than it is to make more requests with smaller messages. To put this into practice in our example database, we may want to add a feature to send multiple <code class="literal">SetRequests</code> or multiple <code class="literal">GetRequests</code> in a single message.</p><p><a id="GBS.0124.05"></a>Remember that testing with components in memory on your local machine is fine in development, but you always need to consider the latency that the network introduces and the impact that it has on the user&#39;s experience. This refers back to one of the reactive tenets: responsiveness. It&#39;s not only real-time systems that this is important for&#8212;the responsiveness of web applications is just as<a id="GBS.0125.01"></a> important as it is in first-person shooters and stock-trading systems.</p></div><div title="Bandwidth is infinite" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"></a>Bandwidth is infinite</h2></div></div></div><p>Bandwidth is increasing <a id="id297" class="indexterm"></a>quite steadily in our networking technology today, so this is a bit less of a concern. Still, we don&#39;t want to send more over the network than we have to. A network has a capacity as well, which must be considered. The more traffic on a network, the more that communication is<a id="GBS.0125.02"></a> impacted by the noise. Also, the more you have to move over the network, the longer it will take.</p><p>For latency, we discussed how our database should handle multiple <code class="literal">SetRequests</code> and <code class="literal">GetRequests</code> in a single message to eliminate the number of requests made over the network. For bandwidth, we strive to reduce the size of the messages sent over the network. Gzip is often used in HTTP communication to<a id="GBS.0125.03"></a> reduce how much data is sent on each request. In large-scale distributed systems, compression algorithms that are easy on CPU (fast) are often used such as Snappy and LZO (for example, Cassandra uses LZO for intra-node communication).</p><p>For our database to take this fallacy&#39;s consideration into effect, we would want to reduce the size of the messages by using compression. Compression isn&#39;t free&#8212;it<a id="GBS.0125.04"></a> has a cost on CPU resources. There are some specialized algorithms that focus on performance rather than compression ratio&#8212;we could use one of these, such as Snappy, to deflate the serialized messages before they are sent to the remote actor and then inflate them again before de-serializing them.</p></div><div title="The network is secure" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"></a>The network is secure</h2></div></div></div><p>It&#39;s easy to <a id="id298" class="indexterm"></a>forget that the networks we operate on are not secure.</p><p>In a<a id="GBS.0125.05"></a> public space like a shared Wi-Fi network, any HTTP traffic can be intercepted and read with minimal technical knowledge. The MySpace sign-on page used to be sent over un-encrypted HTTP, so it was very common for people to sit in cafes on Wi-Fi and steal your credentials. Even HTTPS can be intercepted and read via a Man-In-The-Middle attack using ARP Spoofing&#8212;you&#39;ll get a certificate error still, so<a id="GBS.0126.01"></a> humans are the weak point in the system.</p><p>Just remember people can always read what you&#39;re sending on the network, so you need to ensure that if it&#39;s sensitive information in transit, it&#39;s protected with secure encryption such as AES-256 and that the keys are handled very carefully.</p><p>To apply this to our database, we&#39;d want to allow encrypted communications to occur and we would want to validate<a id="GBS.0126.02"></a> the identity of the sender.</p></div><div title="Network topology doesn&#39;t change" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"></a>Network topology doesn&#39;t change</h2></div></div></div><p>When you build an <a id="id299" class="indexterm"></a>application and deploy it into your company&#39;s infrastructure or on the cloud, today your IPs looks one way but it is out of your control. Depending on specific IPs asserts that the network will never change. This problem is exacerbated when you move to the cloud, where stopping and starting your machine means it gets<a id="GBS.0126.03"></a> a new IP.</p><p>You can work to reduce this impact by using DNS or service registries built with technologies like Zookeeper, but it&#39;s important to assume your network will change.</p></div><div title="There is one administrator" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"></a>There is one administrator</h2></div></div></div><p>I&#39;ve been in trusted positions <a id="id300" class="indexterm"></a>with access to production systems for major brands on a few occasions&#8212;often because I needed to do work in them. You, as the developer, may not realize how many<a id="GBS.0126.04"></a> people will walk in and out of your network and machines. It&#39;s possible you work with an external datacenter or in cloud infrastructure&#8212;remember people can leave the company and different disciplines require multiple people access the box, and so on.</p><p>Why would you care? The other day I deployed some software in test and immediately found it was broken after the application started. I was not aware<a id="GBS.0126.05"></a> that someone had logged into the system and changed some configuration. While managing your software, you should try to eliminate as many touch points as you can&#8212;especially around configuration. Not everyone will have the tools or the understanding of the impacts that changes can incur. A simple change to one component can have unexpected outcomes for other systems.</p></div><div title="Transport cost is zero" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec51"></a>Transport cost is zero</h2></div></div></div><p><a id="GBS.0127.01"></a>There are costs in<a id="id301" class="indexterm"></a> pushing data down to the transport layer. There is a resource cost in serialization of data, compression of that data, pushing it to TCP buffers, and to establish connections. No chatter over the network is free.</p><p>Both in networks and in cloud infrastructure, there is an actual money cost to use the network as well. There is a cost in maintenance in infrastructure, and there is<a id="GBS.0127.02"></a> a cost incurred in use in the cloud.</p></div><div title="The network is homogeneous" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec52"></a>The network is homogeneous</h2></div></div></div><p>In your <a id="id302" class="indexterm"></a>mind, imagine a network. It has communications coming into a firewall from the Internet, which is forwarded to a load balancer. That traffic is then forwarded to your application running on a<a id="id303" class="indexterm"></a> <span class="strong"><strong>Virtual Machine</strong></span> (<span class="strong"><strong>VM</strong></span>). Now imagine you build and deploy the exact same application in another environment.</p><p>Is the second network the<a id="GBS.0127.03"></a> same? Likely, every last component is different. Your firewall might be from Cisco or it might be a home router. Your load balancer might be from F5 or it might be a copy of nginx running on a VM. Your application may be running on an IBM P595 or it may be running on a VMare ESXi host. The VM might be running Ubuntu for an OS or it might be running IBM AIX Unix.</p><p>Does the load <a id="id304" class="indexterm"></a>balancer round robin<a id="GBS.0127.04"></a> or does it sticky sessions? Does the load balancer even support sticky sessions? These are all important questions to ask. I&#39;ve seen an application deployed into production where a load balancer in an early cloud provider didn&#39;t support sticky sessions. A stateful application was deployed without state replication. This was unplanned for as we assumed the network was homogeneous; we assumed all<a id="GBS.0127.05"></a> load balancers were the same&#8212;they do NOT all have the same feature sets.</p></div></div></div><div style="display:none;"><a id="GBS.0127.06"></a></div></body></html>