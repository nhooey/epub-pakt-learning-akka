<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Working with Dispatchers</title><link rel="stylesheet" href="epub.css" type="text/css"></link><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></meta></head><body id="page"><div style="display:none;"><a id="GBS.0162.01"></a></div><div title="Working with Dispatchers" class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"></a>Working with Dispatchers</h1></div></div></div><p>As we start to try to improve <a id="id391" class="indexterm"></a>throughput and response times of our applications, we need to understand all bottlenecks and where time is being spent in the request/response cycle. Once we apply load to an application, the threads that are available will be trying to serve all requests&#8212;understanding how those resources are used will help you improve how much throughput<a id="GBS.0162.02"></a> a service can handle with minimum latency.</p><div title="Dispatchers explained" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"></a>Dispatchers explained</h2></div></div></div><p>A dispatcher decouples<a id="id392" class="indexterm"></a> a task from how and where the task will be run. Dispatchers will generally contain some threads and will handle scheduling and running events such as actor message handling and future events in those threads. Dispatchers are really what make Akka tick—they are the mechanism that gets the work done.</p><p>Any time<a id="GBS.0162.03"></a> an actor or a future does work, the resources allocated by an executor/dispatcher are what does that work.</p><div class="mediaobject"><img src="graphics/B04006_05_02.jpg" alt="Dispatchers explained"></img><a id="GBS.0162.04"></a></div><p>Dispatchers control assigning work to actors. They also can assign resources to handle callbacks on futures. You&#39;ll note that future APIs accept Executors/ExecutionContexts as parameters. Because Akka dispatchers extend these APIs, the dispatchers can pull double duty.</p><p>In Akka, dispatchers implement the <code class="literal">scala.concurrent.ExecutionContextExecutor</code> interface, which, in turn, extends <code class="literal">java.util.concurrent.Executor</code><a id="GBS.0163.01"></a> and <code class="literal">scala.concurrent.ExecutionContext</code>. Executors can be passed to Java futures and ExecutionContexts can be passed to Scala futures.</p><p>For use with futures, dispatchers can be obtained from an <code class="literal">ActorSystem</code> reference (<code class="literal">ActorSystem.dispatcher</code>). To get a dispatcher defined in configuration, you can look them up from the actor system by ID:</p><div class="informalexample"><pre class="programlisting">system.dispatcher //actor system&#39;s dispatcher
system.dispatchers.lookup(&quot;my-disp<a id="GBS.0163.02"></a>atcher&quot;); //custom dispatcher</pre></div><p>Because we can<a id="id393" class="indexterm"></a> create and obtain these executor-backed dispatchers, we can use them to define thread-pools/fork-join pools to separate, isolate, and run our work in. We&#39;ll look at when and why we would want to do this shortly. While you don&#39;t need to understand all of the details of the executors to use them effectively, we will visit them in some detail first.</p></div><div title="Executors" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"></a>Executors</h2></div></div></div><p><a id="GBS.0163.03"></a>Dispatchers are backed by <a id="id394" class="indexterm"></a>executors, so before we look more closely at dispatchers, we&#39;ll cover the two main executor types—<code class="literal">ForkJoinPool</code> and <code class="literal">ThreadPool</code>.</p><p>The thread-pool executor has a queue of work which is assigned to threads. The threads take work as they become free. Threadpools offer a greater efficiency over creating and destroying threads as they allow reuse of threads that are an expensive<a id="GBS.0163.04"></a> resource to create and destroy.</p><p>The &quot;fork-join-pool&quot; executor uses a divide-and-conquer algorithm to recursively split tasks into smaller pieces and then assigns the work to be run on different threads. The results are then combined. While the tasks we are submitting may not be recursively split as <code class="literal">ForkJoinTasks</code>, the fork-join-pool executor has a work-stealing algorithm that allows an idle thread<a id="GBS.0163.05"></a> to &quot;steal&quot; work scheduled for another thread. Work tends to not distribute and complete evenly, so work stealing will more efficiently utilize hardware.</p><p>Fork-join will almost always perform better than the thread-pool executor—it should be your default choice.</p></div><div title="Creating Dispatchers" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"></a>Creating Dispatchers</h2></div></div></div><p>To define a <a id="id395" class="indexterm"></a>dispatcher in <code class="literal">application.conf</code>, we need to specify the dispatcher type and the executor. We can also<a id="GBS.0164.01"></a> specify any configuration-specific details of the executor such as how many threads to use or how many messages to process for each actor before moving on:</p><div class="informalexample"><pre class="programlisting">my-dispatcher { type=Dispatcher executor = &quot;fork-join-executor&quot; fork-join-executor { parallelism-min = 2 #Minimum threads parallelism-factor = 2.0 #Maximum threads per core parallelism-max = 10 #Maximum total threads } throughput = 100 #Max<a id="GBS.0164.02"></a> messages to process in an actor before moving on.
}</pre></div><p>There are four types of dispatchers that can be used which describe how threads are shared among actors.</p><div class="itemizedlist"><ul class="itemizedlist"><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>Dispatchers</strong></span>: Default<a id="id396" class="indexterm"></a> dispatcher type. The defined executor will be used to process messages in actors. This should provide optimal performance in most cases.</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>PinnedDispatcher</strong></span>: This gives<a id="id397" class="indexterm"></a> each actor its own dedicated thread. This executor creates<a id="GBS.0164.03"></a> a thread-pool executor for each actor with each executor having exactly one thread. This may sound like a good idea if you want to ensure that an actor always responds immediately, but there are very few use cases where a pinned dispatcher will perform better than sharing resources. You can try this if a single actor has to do a lot of important work; otherwise, pass on this one.</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>CallingThreadDispatcher</strong></span><a id="GBS.0164.04"></a>: This <a id="id398" class="indexterm"></a>dispatcher is unique in that it has no executor. Instead, the work is run on the calling thread. Its primary use is in testing, especially in debugging. Because the calling thread does the work, there is a clear stack trace that shows the complete context of the executed method that is useful for understanding exceptions. A lock is still obtained in the actor, so only one thread can execute<a id="GBS.0164.05"></a> code in the actor at once, but multiple threads sending messages to an actor will cause all but one thread to wait for a lock. The <code class="literal">CallingThreadDispatcher</code> is how the <code class="literal">TestActorRef </code>is able to do work synchronously in tests as demonstrated earlier in this book.</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>BalancingDispatcher</strong></span>: You <a id="id399" class="indexterm"></a>will see the <code class="literal">BalancingDispatcher</code> referenced in some Akka documentation&#8212;its direct use has been deprecated and replaced<a id="GBS.0165.01"></a> by the <code class="literal">BalancingPool</code> router mentioned earlier. <code class="literal">BalancingDispatcher</code> is still used in Akka, but it should only be used indirectly by a router. We will look at the <code class="literal">BalancingPool</code> in action in the Dispatcher section. The <code class="literal">BalancingDispatcher</code> is unique in that it shares a mailbox with all actors in the pool and optimally creates one thread per actor in the pool. The <code class="literal">BalancingDispatcher</code> Actors pull messages<a id="GBS.0165.02"></a> from the mailbox so that there is never a queue in one actor while another actor is idle. This is a variation on work stealing as all the actors pull from a shared mailbox&#8212;it has a similar beneficial effect on performance.</li></ul></div><p>Actors can be created <a id="id400" class="indexterm"></a>with dispatchers that have been configured by building Props referencing the dispatcher name configured in <code class="literal">application.conf</code>:</p><div class="informalexample"><pre class="programlisting">system.actorOf(Props[MyActor].withDispatcher(<a id="GBS.0165.03"></a>&quot;my-pinned-dispatcher&quot;))</pre></div></div><div title="Deciding Which Dispatcher to use where" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec64"></a>Deciding Which Dispatcher to use where</h2></div></div></div><p>We&#39;ve now covered how to create dispatchers <a id="id401" class="indexterm"></a>and executors, but we don&#39;t really have a clear picture of what to do with them. The purpose of this chapter is to cover how to best utilize our hardware, so we&#39;re now going to look at how we can use dispatchers to produce an application that is more responsive to our users by being more<a id="GBS.0165.04"></a> resilient to potential performance problems.</p><p>We&#39;ll skip back to our example of an application that extracts article bodies from web pages and caches them, and that also serves cached articles to a user. For our example, let&#39;s assume that the user profiles are retrieved from an RDBMS requiring thread-blocking JDBC calls. This introduces blocking of our limited threads to aid in highlighting what<a id="GBS.0165.05"></a> we&#39;re trying to accomplish.</p><p>The first step to scaling up is to understand which use cases are most important to serve immediately and where there may be contention for the resources needed to serve those important requests. If we use only the default dispatcher, and 1,000 requests come in consisting of 500 blocking (for example, JDBC) and 500 non-blocking operations, we don&#39;t want the blocking<a id="GBS.0166.01"></a> operations to tie up all the threads needed to serve the important requests.</p><div class="mediaobject"><img src="graphics/B04006_05_03.jpg" alt="Deciding Which Dispatcher to use where"></img><a id="GBS.0166.02"></a></div><p>In the preceding<a id="id402" class="indexterm"></a> diagram, we simplify the scale of this example to show 8 threads being used by longer running tasks such as blocking IO and heavier processing. The diagram shows the impact that it has on other requests that are waiting to be processed because all of the threads available are tied up. The other requests cannot begin processing until those resources are freed. The waiting tasks<a id="GBS.0166.03"></a> might just need to do short-lived cache reads, but they will have to wait in line for resources to become free to do the work.</p><p>This highlights that having one pool of resources distributed without order can allow riskier areas of the application to tie up resources that we need to give to our important primary use cases.</p><p>We can do better than this by isolating the resources in the areas of risk<a id="GBS.0166.04"></a> from competing with those that serve the important tasks. If we create new dispatchers and assign any long-running or blocking work to those dispatchers, we can ensure that the rest of our application remains responsive. We want to separate all of the heavy and long-running work into independent dispatchers to ensure that resources are available for other tasks in adverse situations.</p><p>By doing<a id="GBS.0166.05"></a> this, we can isolate the delays to those areas of the application. If MySQL goes on the fritz and starts taking 30 seconds to respond, at least other paths through the application will remain responsive.</p><p>This approach requires us to first examine our application&#39;s performance, understanding where the application can block and tie up resources. We need to categorize the work done in the application.</p><p><a id="GBS.0167.01"></a>If we look at our example case, we might categorize work it into the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>Article Parsing</strong></span>: Longer running CPU-intensive tasks (10% of requests)</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>JDBC Profile Reads from MySQL</strong></span>: Longer running thread-blocking IO (10% of requests)</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>Article Retrieval</strong></span>: Simple non-blocking requests to get articles from remote in-memory datastore (80% of requests)</li></ul></div><p>Once we have a <a id="id403" class="indexterm"></a>picture of the types of work that the<a id="GBS.0167.02"></a> application is doing, then we want to understand if there are performance risks there&#8212;is it possible that something could cause resource utilization to spiral out of control, affecting the rest of the application? Look at both thread blocking and CPU-intensive work and evaluate if it&#39;s possible for them to cause some resource starvation in other important areas of the application:</p><div class="itemizedlist"><ul class="itemizedlist"><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>Article Parsing</strong></span><a id="GBS.0167.03"></a>: If someone submits several large books that have been posted online, all threads could be used up in very intensive long-running work. It is of moderate risk and can be mitigated by limiting the size of submissions.</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>JDBC Profile Reads</strong></span>: If the database starts taking 30 seconds to respond, all threads could be used up waiting. It is of higher risk.</li><li style="list-style-type: disc;" class="listitem"><span class="strong"><strong>Article Retrieval</strong></span>: Article retrieval does not block<a id="GBS.0167.04"></a> and does no heavy work, so it is low-risk. It is also important to serve this traffic fast as it will account for most of the traffic.</li></ul></div><p>Now that we have identified where higher risk activities might be, we want to isolate any portion of work that has risk into its own dispatcher so that if/when those conditions occur, our application will not be negatively affected in other important areas. This<a id="GBS.0167.05"></a> is another example of bulkheading to isolate the impact of failure. Just a reminder, never make assumptions and always measure how changes affect your system&#39;s performance!</p><p>Now that we have categorized our application&#39;s work and risk front, we can take any high risk areas, and isolate them into their own dispatcher. The approach we will use will look like the following:</p><div class="mediaobject"><img src="graphics/B04006_05_04.jpg" alt="Deciding Which Dispatcher to use where"></img><a id="GBS.0168.01"></a></div><p>Non-Blocking Article Reads and Akka will be okay to run in the default dispatcher at this point. If we discover other risks, then we can further separate the work later, but this is a good starting point.</p><p>Blocking IO—for <a id="id404" class="indexterm"></a>example, JDBC Profile Reads for profile information&#8212;will get its own dispatcher with 50 or 100 threads&#8212;work that blocks and waits for IO (that is from a database with JDBC drivers)<a id="GBS.0168.02"></a> should be isolated from asynchronous threadpools because the work will halt all other operations in the application if all of the threads get tied up waiting for IO. This is probably the most critical of the changes&#8212;we should always try to place blocking IO outside of Akka&#39;s dispatcher.</p><p>Article Parsing will get its own dispatcher with a small number of threads. We&#39;re using the dispatcher for isolation<a id="GBS.0168.03"></a> here incase very big jobs are submitted. If big jobs are submitted, then the work will halt any other work in the queue; thus, to protect against those exceptional cases, we can isolate the work. In this case, we can also use the <code class="literal">BalancingPool</code>/<code class="literal">BalancingDispatcher</code> to distribute work across a pool of article parsing actors. In addition to giving isolation, using the <code class="literal">BalancingPool</code> also gives a potential<a id="GBS.0168.04"></a> improvement in resource utilization by the <code class="literal">BalancingPool</code> work-stealing nature.</p></div><div title="Default Dispatcher" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec65"></a>Default Dispatcher</h2></div></div></div><p>There are a couple of approaches that we can take with the <a id="id405" class="indexterm"></a>default dispatcher. We can either separate all work out, leaving it only for Akka to use, or we can check that only async work gets done in the default dispatcher and move any higher-risk work out. Either way, we should never block in the<a id="GBS.0168.05"></a> default dispatcher and have to be a bit careful with the work that is run in it to prevent resource starvation.</p><p>You don&#39;t need to do anything to create or use the default dispatcher/threadpool. We&#39;re done. If needed, we can configure the default dispatcher by defining it in the <code class="literal">application.conf</code> file on the classpath similar to the following:</p><div class="informalexample"><pre class="programlisting">akka { actor { default-dispatcher { # Min number of<a id="GBS.0169.01"></a> threads to cap factor-based parallelism number to parallelism-min = 8
        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values. parallelism-factor = 3.0
        # Max number of threads to cap factor-based parallelism number<a id="GBS.0169.02"></a> to parallelism-max = 64
        # Throughput for default Dispatcher, set to 1 for as fair as possible
        throughput = 10
    }
  }
}</pre></div><p>You can override default configuration values by defining any values again in your own <code class="literal">application.conf</code> file:</p><div class="informalexample"><pre class="programlisting">akka { actor { default-dispatcher { # Throughput for default Dispatcher, set to 1 for as fair as possible throughput = 1
    }
  }
}</pre></div><p>By default, all<a id="GBS.0169.03"></a> work done by Actors will execute in this dispatcher. If you need to get the <code class="literal">ExecutionContext</code> to create futures in, you can access the default threadpool through your ActorSystem, and then supply it to futures:</p><div class="informalexample"><pre class="programlisting">ActorSystem system = ActorSystem.create();
CompletableFuture.runAsync(() -&gt;System.out.println(&quot;run in ec&quot;), system.dispatcher());
val system = ActorSystem()
implicit val ec = system.dispatcher<a id="GBS.0169.04"></a>
val future = Future(() =&gt;println(&quot;run in ec&quot;))</pre></div><div title="Note" style="" class="note"><div class="inner"><h3 class="title"><a id="tip10"></a>Tip</h3><p>Note: Be careful about the work you do in futures in the default dispatcher as it will take time away from the actors themselves. We&#39;ll look at how to remedy this in the next section.</p></div></div><p>In Scala, classes <a id="id406" class="indexterm"></a>extending Actor already have the dispatcher as an implicit val, so you do not have to specify it if working with futures in actors. There are<a id="GBS.0169.05"></a> few cases where you want to have futures in your actors&#8212;remember that we should prefer <code class="literal">tell</code> over <code class="literal">ask</code> so you might want to evaluate your approaches if you&#39;re finding you&#39;re working with Futures a lot inside your actors.</p><p>That covers the default dispatcher and how to use it and tune it. Now, we&#39;ll look at how to add and use additional executors.</p></div><div title="Blocking IO dispatcher use with futures" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec66"></a>Blocking IO dispatcher use with futures</h2></div></div></div><p>If you have<a id="GBS.0170.01"></a> any<a id="id407" class="indexterm"></a> blocking work, you want to get it out<a id="id408" class="indexterm"></a> of the main dispatcher so that actors can stay lively if your app gets saturated with blocking work.</p><p>Consider this use case: a user wants to retrieve a user profile from an RDBMS to see who posted an article. We might have a service interface that looks like the following:</p><div class="informalexample"><pre class="programlisting">import org.springframework.data.repository.CrudRepository;
import java.util.List;<a id="GBS.0170.02"></a>
public interface UserProfileRepository extends CrudRepository&lt;UserProfile, Long&gt; { List&lt;UserProfile&gt;findById(Long id); }</pre></div><p>For this example, we&#39;ll use Spring Data to access a database via JDBC. Spring Data is an arbitrary choice here for use in the example for a blocking API&#8212;it lets us demonstrate blocking IO with minimal code, but the approach could be for anything that blocks threads. This interface<a id="GBS.0170.03"></a> is actually all the code that you need to query for JPA-annotated <code class="literal">UserProfiles</code>—Spring supplies the implementations for you by analyzing the method name.</p><p>For our example, we have a method called <code class="literal">findById</code> that accepts an ID and blocks the calling thread while waiting for IO to come back from the database.</p><p>If we call this method in an actor, we&#39;re tying up a thread in our default dispatcher, not<a id="GBS.0170.04"></a> to mention halting the actor from doing any other work:</p><div class="informalexample"><pre class="programlisting">  //Java sender().tell(userProfileRepository.findById(id), self());
  //Scala
  sender() ! userProfileRepository.findById(id)</pre></div><p>Again, if we have <a id="id409" class="indexterm"></a>several requests that come in and hit that block<a id="id410" class="indexterm"></a> of code (maybe in a pool of actors), all the threads can be stuck waiting so that no other work is able to move forward until resources are freed.</p><p><a id="GBS.0170.05"></a>The simplest solution in these cases is to run the blocking operations in another set of threads in a different dispatcher.</p><p>First, in our <code class="literal">application.conf</code> file we&#39;ll create a dispatcher with a bigger pool of resources:</p><div class="informalexample"><pre class="programlisting">blocking-io-dispatcher { type = Dispatcher executor = &quot;fork-join-executor&quot; fork-join-executor { parallelism-factor = 50.0
    parallelism-min = 10
    parallelism-max = 100
  }
}</pre></div><p><a id="GBS.0171.01"></a>That will allow up to 50 threads per core to be created with a minimum of 10 and a maximum of 100 threads. Hundred threads is a large upper limit for a properly factored and indexed database.</p><p>For blocking IO to databases, if you have queries that take a long time to run, you should examine the run plans and fix your tables and queries instead of adding more threads. Each thread has a memory overhead,<a id="GBS.0171.02"></a> so don&#39;t arbitrarily add more threads. Measure, change, and repeat until optimal, adjusting the threadpool only after your queries, tables, and indices are optimized.</p><p>Now that we have a dispatcher configured, we need to gain access to it to be able to run the blocking queries in it. We can get a reference to a dispatcher by looking up the dispatcher from the actor system. In an actor, we would<a id="GBS.0171.03"></a> call the following:</p><div class="informalexample"><pre class="programlisting">//Java
val ec: ExecutionContext = context.system.dispatchers.lookup(&quot;blocking-io-dispatcher&quot;)
//Scala
Executor ex = context().system().dispatchers().lookup(&quot;blocking-io-dispatcher&quot;);</pre></div><p>Once we have the dispatcher references, we can use them with the future APIs to run the work there:</p><div class="informalexample"><pre class="programlisting">//Java
CompletableFuture&lt;UserProfile&gt; future = CompletableFuture.supplyAsync( () -&gt;userProfileRepository.findByI<a id="GBS.0171.04"></a>d(id) , ex);
//Scala
val future: Future[UserProfile] = Future{ userProfileRepository.findById(id)
  }(ec)</pre></div><p>In each of the<a id="id411" class="indexterm"></a> Scala and Java future APIs, all that we <a id="id412" class="indexterm"></a>have to do is supply the dispatcher reference as the second parameter of the future and the dispatcher will take care of the rest. Once the result is available, the future will complete.</p><p>Having the future reference, we can now use it as<a id="GBS.0171.05"></a> we normally would in an actor&#8212;likely using <code class="literal">patterns.Pipe</code> to send the result to another actor asynchronously.</p><p>This use of futures is an important technique&#8212;blocking IO will very quickly ruin your application&#39;s performance. I would recommend that you try to use non-blocking drivers instead of doing this, but if you need to use blocking drivers with chosen technologies, then this is a reasonable<a id="GBS.0172.01"></a> approach.</p><p>In the same manner as with blocking IO, any heavy computation done with results from futures can also be moved to another dispatcher to help actors stay lively.</p></div><div title="Article parsing dispatcher" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec67"></a>Article parsing dispatcher</h2></div></div></div><p>For our last example, we&#39;ll <a id="id413" class="indexterm"></a>look at how we can assign actors to another dispatcher. This is different than taking only a piece of a task and running it in another dispatcher, like we saw in the JDBC<a id="GBS.0172.02"></a> example, because we&#39;re actually assigning actors completely to another dispatcher instead of just some work over there. This is well suited for any actors that do heavier processing.</p><p>We&#39;re going to look at two options:</p><div class="itemizedlist"><ul class="itemizedlist"><li style="list-style-type: disc;" class="listitem">Defining a Dispatcher to use with an actor pool</li><li style="list-style-type: disc;" class="listitem">Using the <code class="literal">BalancingPool</code> router that uses <code class="literal">BalancingDispatcher</code></li></ul></div><div title="Using a configured dispatcher with Actors" class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec61"></a>Using a configured dispatcher with Actors</h3></div></div></div><p>Here we&#39;ll look at <a id="id414" class="indexterm"></a>configuring<a id="GBS.0172.03"></a> a dispatcher in our <code class="literal">application.conf</code>, and then assigning actors to that dispatcher when we create them. This<a id="id415" class="indexterm"></a> is fairly straightforward and does not vary much from many of the other activities we&#39;ve looked at so far.</p><p>First, we&#39;ll create another dispatcher in our <code class="literal">application.conf</code> for the article parsing. We&#39;ll assign a smaller number of threads this time:</p><div class="informalexample"><pre class="programlisting">article-parsing-dispatcher { # Dispatcher<a id="GBS.0172.04"></a> is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = &quot;fork-join-executor&quot;
  # Configuration for the fork join pool
  fork-join-executor { # Min number of threads to cap factor-based parallelism number to parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads<a id="GBS.0172.05"></a> to cap factor-based parallelism number to parallelism-max = 8 }
  throughput = 50
}</pre></div><p>Now, to create actors assigned to the configured dispatcher, we simply call the <code class="literal">withDispatcher</code> method while creating the Props. We&#39;ll use a range to create a list of actors, and then place them in the dispatcher:</p><div class="informalexample"><pre class="programlisting">//Java
        List&lt;ActorRef&gt;routees = Arrays.asList(1,2,3,4,5,6,7,8).stream().map(x -&gt; system.actorOf(Props.create(Ar<a id="GBS.0173.01"></a>ticleParseActor.class). withDispatcher(&quot;article-parsing-dispatcher&quot;))
        ).collect(Collectors.toList());  
//Scala
val actors: List[ActorRef] = (0 to 7).map(x =&gt; { system.actorOf(Props(classOf[ArticleParseActor]). withDispatcher(&quot;article-parsing-dispatcher&quot;)) }).toList</pre></div><p>Now, we can do anything we like with actors created this way. For example, we can produce a router to use the actors so that<a id="GBS.0173.02"></a> we can easily do work in parallel with them:</p><div class="informalexample"><pre class="programlisting">//Java
Iterable&lt;String&gt;routeeAddresses = routees. stream().
                map(x -&gt;x.path().toStringWithoutAddress()).
                collect(Collectors.toList());
ActorRefworkerRouter = system.actorOf(new RoundRobinGroup(routeeAddresses).props());
//Scala valworkerRouter = system.actorOf(RoundRobinGroup( actors.map(x =&gt;x.path.toStringWithoutAddress).toList).props()<a id="GBS.0173.03"></a>, &quot;workerRouter&quot;)
workRouter.tell( new ParseArticle(TestHelper.file) , self());</pre></div><p>This is a slightly different syntax for creating a router actor than we saw earlier. We saw an example of a pool—where the router creates the routee actors. Here, we are taking pre-existing actors that we created previously and creating a router actor by passing those actors in using a group load-balancing strategy (<code class="literal"><a id="GBS.0173.04"></a>RoundRobinGroup</code>). The router groups take a list of addresses, and then produce the props you need to produce the router actor. This is very <a id="id416" class="indexterm"></a>similar to what we would do if the actors <a id="id417" class="indexterm"></a>were remote as well.</p><div title="Note" style="" class="note"><div class="inner"><h3 class="title"><a id="tip11"></a>Tip</h3><p>Note: The Group can also take the name of a dispatcher if you want the router to be assigned to a dispatcher as well.</p></div></div></div><div title="Using BalancingPool/BalancingDispatcher" class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec62"></a>Using BalancingPool/BalancingDispatcher</h3></div></div></div><p>Because the actors are local,<a id="GBS.0173.05"></a> we<a id="id418" class="indexterm"></a> have a better option than using the <code class="literal">RoundRobinGroup</code> shown<a id="id419" class="indexterm"></a> previously. For local actors, we can create a router with a <code class="literal">BalancingPool</code>, which was briefly described earlier in this chapter. The <code class="literal">BalancingPool</code> will share a single mailbox across all actors in the pool and effectively offer &quot;work-stealing&quot; mechanics to re-distribute load to any idle actors. Using a <code class="literal">BalancingPool</code> helps ensure there<a id="GBS.0174.01"></a> are no idle actors when there is work to do because all actors pull messages from the same mailbox. Technically, it&#39;s not work stealing as the router is not re-assigning work like <code class="literal">ForkJoinPool</code> does&#8212;it&#39;s just that idle actors will pick up the next message from the shared mailbox. The end result is the same&#8212;there is no possibility for one actor to have several messages queued while another actor<a id="GBS.0174.02"></a> has none. As we can ensure more actors are actively working, this option can often lead to better resource utilization than the other balancing strategies.</p><p>The <code class="literal">BalancingPool</code> uses a special dispatcher—<code class="literal">BalancingDispatcher</code>. In most cases, we want the <code class="literal">BalancingDispatcher</code> to have a number of threads equivalent to the number of actors that are used.</p><p>First, we&#39;ll configure the default <code class="literal">BalancingDispatcher</code><a id="GBS.0174.03"></a> executors in the <code class="literal">application.conf</code> to have exactly 8 threads:</p><div class="informalexample"><pre class="programlisting">//Dispatcher for BalancingPool
pool-dispatcher { fork-join-executor { # force it to allocate exactly 8 threads parallelism-min = 8 parallelism-max = 8 }
}</pre></div><p>Then, we make a pool of eight actors—the same number as the number of threads in the pool dispatcher:</p><div class="informalexample"><pre class="programlisting">//Java ActorRefworkerRouter = system.actorOf(new BalancingPool(8).props(Props.create(ArticleParse<a id="GBS.0174.04"></a>Actor.class)), &quot;balancing-pool-router&quot;);
//Scala valworkerRouter = system.actorOf(BalancingPool(8).props(Props(classOf[ArticleParseActor])),
      &quot;balancing-pool-router&quot;)</pre></div><p>This is a great way of <a id="id420" class="indexterm"></a>ensuring <a id="id421" class="indexterm"></a>work is balanced across all actors when working locally.</p></div></div><div title="Optimal parallelism" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec68"></a>Optimal parallelism</h2></div></div></div><p>There is only one way to determine<a id="id422" class="indexterm"></a> for certain what the optimal level of parallelism is on your hardware: measuring.<a id="GBS.0174.05"></a> You&#39;ll almost always make incorrect assumptions about where time is spent and how changes impact systems until you actually measure and adjust.</p><p>Using a number of threads far greater than the number of cores in your processor will reduce performance, so don&#39;t arbitrarily choose large pools of actors and assume more is better. Akka has to switch processing between actors, and your executors need<a id="GBS.0175.01"></a> to balance work between threads. Your OS has to schedule CPU time to the active threads and context-switch by swapping the state of the active thread in and out. Because of the overhead, optimal parallelism for the fastest processing times may actually come in lower than you expect. The only way to know for sure what impact a change has is to measure!</p></div></div><div style="display:none;"><a id="GBS.0175.02"></a></div></body></html>