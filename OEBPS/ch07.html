<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 7. Handling Mailbox Problems</title><link rel="stylesheet" href="epub.css" type="text/css"></link><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></meta></head><body id="page"><div style="display:none;"><a id="GBS.0216.01"></a></div><div title="Chapter 7. Handling Mailbox Problems" class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"></a>Chapter 7. Handling Mailbox Problems</h1></div></div></div><p>Congratulations! You&#39;ve made it through all of the tough content now. You&#39;ve learned Akka, you&#39;ve learned how to scale it, and how to describe your system&#39;s behavior in those situations!</p><p>In this chapter, we will look at what happens when you start to hit the limits of your actor system and how to describe how your system should behave in those situations.</p><p><a id="GBS.0216.02"></a>Let&#39;s start by setting the stage for the problem and then look at different approaches we can use to overcome these issues.</p><div title="Overwhelming your weakest link" class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"></a>Overwhelming your weakest link</h1></div></div></div><p>To continue our running example, imagine we have an application that extracts articles and stores the body in our key-value store. The extracted articles are then displayed on various devices that have a reader application.</p><p>You&#39;ve launched<a id="GBS.0216.03"></a> the application and have a growing user base. Everything is working fine. People can request articles to read from the device on their application; it hits a public REST API, which makes a request to the parse service. The parse service will check the store and, if the article has not been parsed, then it will parse it and then cache it.</p><p>The following figure represents the flow of articles as<a id="GBS.0216.04"></a> they are parsed and stored:</p><div class="mediaobject"><img src="graphics/B04006_07_01.jpg" alt="Overwhelming your weakest link"></img><a id="GBS.0216.05"></a></div><p>Let&#39;s imagine a day when several good events happen. Our app shows up in the number 1 spot, for example on Hacker News, all day and we get 10x more traffic than we have ever seen.</p><p>First, we start to see timeouts in the REST API application. Response times balloon when making requests from the API. Eventually, the article parse services crash with out-of-memory errors, so we start to analyze the<a id="GBS.0217.01"></a> traffic in the application under load. Because article parsing is the most processing intensive piece of the application, lots of messages will get queued there. This is the slowest service in our data flow.</p><div class="mediaobject"><img src="graphics/B04006_07_02.jpg" alt="Overwhelming your weakest link"></img><a id="GBS.0217.02"></a></div><p>Remember that messages go into a mailbox in memory. If a service is consuming messages slower than the rate at which they are delivered, then mailboxes will become more and more full. Let&#39;s look at the events that occurred when we saw the load, in a bit more detail to understand why they occur. We can look at different ways in which we can configure our application to make sure it stays up the<a id="GBS.0217.03"></a> next time we&#39;re overwhelmed like this.</p><div title="Ballooning response times" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec83"></a>Ballooning response times</h2></div></div></div><p>The first thing we&#39;ll see when we start<a id="id498" class="indexterm"></a> overwhelming our slow article parse worker consumer is increasing response times. If our ArticleParser can process 100 articles in a second, and we send it 101 articles a second, then messages will slowly accumulate. After 1,000 seconds, there will be 1,000 messages queued up in the mailbox.</p><p><a id="GBS.0217.04"></a>Once there is a queue of messages, any messages that have been received first will need to be processed before any new messages can be processed, so what we will see is response times start to grow.</p><p>The more these unhandled messages are queued, the longer it takes to process new messages.</p><div class="mediaobject"><img src="graphics/B04006_07_03.jpg" alt="Ballooning response times"></img><a id="GBS.0218.01"></a></div><p>A slow consumer of <a id="id499" class="indexterm"></a>messages will cause downstream problems as well. If you have a very fast consumer behind a slow consumer, your response times will be primarily affected by your slowest consumer. So we have to focus on the slowest consumer to make our system respond better. One of our goals in a reactive system design is to ensure it is always responsive, so this violates the responsiveness<a id="GBS.0218.02"></a> principle.</p></div><div title="Crashing" class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec84"></a>Crashing</h2></div></div></div><p>If we continue to try to push <a id="id500" class="indexterm"></a>more and more messages through the system, we&#39;ll eventually have a very large number of messages in the slow message consumer&#39;s mailbox, which causes a far worse problem to occur—crashes due to out-of-memory errors.</p><p>Once response times get to this point, people trying to use the service are probably going to be hitting retry again and again,<a id="GBS.0218.03"></a> piling even more messages into the mailbox of our already overburdened services.</p><p>The default mailbox is unbounded, meaning that the service will accumulate messages indefinitely. Resources are limited&#8212;the JVM only has so much memory available&#8212;so eventually the slow message consumer will have so many messages in memory that the JVM will not have the memory needed to create new objects and will<a id="GBS.0218.04"></a> crash.</p><div class="mediaobject"><img src="graphics/B04006_07_04.jpg" alt="Crashing"></img><a id="GBS.0218.05"></a></div><p>If we have a cluster of <a id="id501" class="indexterm"></a>workers, as demonstrated in the previous chapter, the crashing of one node in the cluster means that other members of the cluster will suddenly start receiving more messages. The effect is usually a cascading one—more messages go to the other nodes and eventually they will crash as well. When the whole system goes down, then we&#39;re completely unavailable.</p></div></div></div><div style="display:none;"><a id="GBS.0219.01"></a></div></body></html>